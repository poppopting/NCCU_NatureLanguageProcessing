{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Guan-Ting\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data 900 (POS) 900 (NEG)\n",
      "Number of test data 100 (POS) 100 (NEG)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "pos_reviews = []\n",
    "neg_reviews = []\n",
    "\n",
    "for label in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(label):\n",
    "        doc = movie_reviews.words(fileid)\n",
    "        if label == 'pos':\n",
    "            pos_reviews.append(\" \".join(doc))\n",
    "        else:\n",
    "            neg_reviews.append(\" \".join(doc))\n",
    "            \n",
    "train_pos_reviews = pos_reviews[:900]\n",
    "train_neg_reviews = neg_reviews[:900]\n",
    "test_pos_reviews = pos_reviews[900:]\n",
    "test_neg_reviews = neg_reviews[900:]\n",
    "\n",
    "print(\"Number of training data %d (POS) %d (NEG)\" % (\n",
    "    len(train_pos_reviews), len(train_neg_reviews)) )\n",
    "print(\"Number of test data %d (POS) %d (NEG)\" % (\n",
    "    len(test_pos_reviews), len(test_neg_reviews)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'films adapted from comic books have had plenty of success , whether they \\' re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there \\' s never really been a comic book like from hell before . for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid \\' 80s with a 12 - part series called the watchmen . to say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . the book ( or \" graphic novel , \" if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . in other words , don \\' t dismiss this film because of its source . if you can get past the whole comic book thing , you might find another stumbling block in from hell \\' s directors , albert and allen hughes . getting the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that \\' s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? the ghetto in question is , of course , whitechapel in 1888 london \\' s east end . it \\' s a filthy , sooty place where the whores ( called \" unfortunates \" ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . when the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . abberline , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . upon arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn \\' t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can \\' t stomach . i don \\' t think anyone needs to be briefed on jack the ripper , so i won \\' t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . in the comic , they don \\' t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . it \\' s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . and from hell \\' s ending had me whistling the stonecutters song from the simpsons for days ( \" who holds back the electric car / who made steve guttenberg a star ? \" ) . don \\' t worry - it \\' ll all make sense when you see it . now onto from hell \\' s appearance : it \\' s certainly dark and bleak enough , and it \\' s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . the print i saw wasn \\' t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don \\' t say a word ) ably captures the dreariness of victorian - era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black - and - white comic . oscar winner martin childs \\' ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . even the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . ians holm ( joe gould \\' s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . i cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn \\' t half bad . the film , however , is all good . 2 : 00 - r for strong violence / gore , sexuality , language and drug content'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = []\n",
    "train_Y = []\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for x in train_pos_reviews:\n",
    "    train_X.append(x)\n",
    "    train_Y.append(1)\n",
    "for x in train_neg_reviews:\n",
    "    train_X.append(x)\n",
    "    train_Y.append(0)\n",
    "    \n",
    "for x in test_pos_reviews:\n",
    "    test_X.append(x)\n",
    "    test_Y.append(1)\n",
    "for x in test_neg_reviews:\n",
    "    test_X.append(x)\n",
    "    test_Y.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Guan-Ting\n",
      "[nltk_data]     Chen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopword_list = stopwords.words('english')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    # remove conjunction and prepositions\n",
    "    filter_tags = ['CC', 'IN']\n",
    "    tokens = word_tokenize(x)\n",
    "    pos_token = nltk.pos_tag(tokens)\n",
    "    cleaned_tokens = []\n",
    "    for (tok, pos) in pos_token:\n",
    "        #remove stopword and keep english letters\n",
    "        if tok.isalpha() & (tok not in stopword_list) & (pos not in filter_tags):\n",
    "            tok_lower = tok.lower()\n",
    "            #steming each word\n",
    "            cleaned_tokens.append(snowball_stemmer.stem(tok_lower))\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean our training data\n",
    "cleaned_train_X = []\n",
    "cleaned_test_X = []\n",
    "for x in train_X:\n",
    "    cleaned_train_X.append(clean(x))\n",
    "for x in test_X:\n",
    "    cleaned_test_X.append(clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use CountVectorizer to calculate the frequency of each word in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = CountVectorizer(max_features = 1500, ngram_range=(1, 3))\n",
    "training_vectors = feature_extractor.fit_transform(cleaned_train_X)\n",
    "test_vectors = feature_extractor.transform(cleaned_test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(i, j) f = the j-th word in word vocabulary occur f times in the i-th sentence\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 494)\t6\n",
      "  (0, 20)\t1\n",
      "  (0, 244)\t5\n",
      "  (0, 142)\t4\n",
      "  (0, 991)\t1\n",
      "  (0, 1290)\t1\n",
      "  (0, 109)\t1\n",
      "  (0, 1233)\t1\n",
      "  (0, 727)\t1\n",
      "  (0, 302)\t1\n",
      "  (0, 559)\t1\n",
      "  (0, 1478)\t2\n",
      "  (0, 897)\t2\n",
      "  (0, 1061)\t2\n",
      "  (0, 610)\t5\n",
      "  (0, 290)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 868)\t3\n",
      "  (0, 397)\t1\n",
      "  (0, 178)\t3\n",
      "  (0, 158)\t1\n",
      "  (0, 1453)\t2\n",
      "  (0, 898)\t1\n",
      "  (0, 769)\t1\n",
      "  (0, 958)\t1\n",
      "  :\t:\n",
      "  (0, 595)\t1\n",
      "  (0, 156)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 709)\t1\n",
      "  (0, 1153)\t1\n",
      "  (0, 580)\t1\n",
      "  (0, 1300)\t1\n",
      "  (0, 1109)\t1\n",
      "  (0, 121)\t1\n",
      "  (0, 937)\t1\n",
      "  (0, 875)\t1\n",
      "  (0, 657)\t1\n",
      "  (0, 86)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 592)\t1\n",
      "  (0, 100)\t1\n",
      "  (0, 641)\t1\n",
      "  (0, 574)\t1\n",
      "  (0, 1174)\t1\n",
      "  (0, 744)\t1\n",
      "  (0, 385)\t1\n",
      "  (0, 269)\t1\n",
      "  (0, 245)\t3\n",
      "  (0, 809)\t1\n",
      "  (0, 505)\t1\n"
     ]
    }
   ],
   "source": [
    "print(training_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 1 : LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define metric f-score to monitor LightGBM model\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat)\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective' :'binary',\n",
    "    'learning_rate' : 0.05,\n",
    "    'num_leaves' : 100,\n",
    "    'feature_fraction': 0.7, \n",
    "    'bagging_fraction': 0.8, \n",
    "    'bagging_freq':1,\n",
    "    'boosting_type' : 'gbdt',\n",
    "    'metric': 'auc'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[50]\tvalid_0's auc: 0.9018\tvalid_0's f1: 0.830918\n",
      "[100]\tvalid_0's auc: 0.918\tvalid_0's f1: 0.84058\n",
      "[150]\tvalid_0's auc: 0.9265\tvalid_0's f1: 0.870813\n",
      "[200]\tvalid_0's auc: 0.9263\tvalid_0's f1: 0.869565\n",
      "[250]\tvalid_0's auc: 0.9295\tvalid_0's f1: 0.878049\n",
      "[300]\tvalid_0's auc: 0.9305\tvalid_0's f1: 0.883495\n",
      "[350]\tvalid_0's auc: 0.9324\tvalid_0's f1: 0.872549\n",
      "[400]\tvalid_0's auc: 0.9317\tvalid_0's f1: 0.872549\n",
      "[450]\tvalid_0's auc: 0.9316\tvalid_0's f1: 0.866995\n",
      "[500]\tvalid_0's auc: 0.9326\tvalid_0's f1: 0.871287\n",
      "[550]\tvalid_0's auc: 0.9329\tvalid_0's f1: 0.878049\n",
      "[600]\tvalid_0's auc: 0.9331\tvalid_0's f1: 0.887805\n",
      "[650]\tvalid_0's auc: 0.9333\tvalid_0's f1: 0.887805\n",
      "[700]\tvalid_0's auc: 0.9318\tvalid_0's f1: 0.882353\n",
      "[750]\tvalid_0's auc: 0.9306\tvalid_0's f1: 0.878049\n",
      "[800]\tvalid_0's auc: 0.9313\tvalid_0's f1: 0.878049\n",
      "Early stopping, best iteration is:\n",
      "[327]\tvalid_0's auc: 0.9317\tvalid_0's f1: 0.888889\n"
     ]
    }
   ],
   "source": [
    "train = lgbm.Dataset(training_vectors.astype(float), train_Y)\n",
    "valid = lgbm.Dataset(test_vectors.astype(float), test_Y)\n",
    "\n",
    "# training with early stop\n",
    "clf = lgbm.train(params, train, num_boost_round=5000, valid_sets=[valid], verbose_eval=50, feval=lgb_f1_score, early_stopping_rounds=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate F-score: 0.884859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.885\tPrecision: 0.886896\tRecall: 0.885\tF-score: 0.884859\n"
     ]
    }
   ],
   "source": [
    "#define probability threshold\n",
    "THRESHOLD = 0.5\n",
    "#################################################################\n",
    "test_prob_Y = clf.predict(test_vectors.astype(float))\n",
    "pred_Y = [1 if i>THRESHOLD else 0 for i in test_prob_Y]\n",
    "accuracy = accuracy_score(test_Y, pred_Y)\n",
    "precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "\n",
    "print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g\" % (\n",
    "    accuracy, precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model2 Logistic  Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty='elasticnet', solver='saga', C=0.6, max_iter=500, l1_ratio=0.2)\n",
    "clf = clf.fit(training_vectors, train_Y)\n",
    "\n",
    "pred_Y = clf.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluate  F-score: 0.834996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.835\tPrecision: 0.835034\tRecall: 0.835\tF-score: 0.834996\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_Y, pred_Y)\n",
    "precision = precision_score(test_Y, pred_Y, average='macro')\n",
    "recall = recall_score(test_Y, pred_Y, average='macro')\n",
    "fscore = f1_score(test_Y, pred_Y, average='macro')\n",
    "\n",
    "print(\"Accuracy: %g\\tPrecision: %g\\tRecall: %g\\tF-score: %g\" % (\n",
    "    accuracy, precision, recall, fscore))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
