{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinese Word Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training/test data is publicly avaialbe here: http://sighan.cs.uchicago.edu/bakeoff2005/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training data: 708953\n",
      "Number of sentences in the test data: 14432\n"
     ]
    }
   ],
   "source": [
    "raw_train = []\n",
    "raw_test = []\n",
    "with open(\"data/as_training.utf8\", encoding=\"utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_train.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "with open(\"data/as_testing_gold.utf8\", encoding=\"utf8\") as fin:\n",
    "    for line in fin:\n",
    "        raw_test.append(line.strip().split(\"　\"))   # It is a full white space\n",
    "\n",
    "print(\"Number of sentences in the training data: %d\" % len(raw_train))\n",
    "print(\"Number of sentences in the test data: %d\" % len(raw_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.564 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區長', '青學苑', '多', '開設', '有書法', '、', '插花', '、', '土風', '舞班', '，']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "print(list(jieba.cut(\"\".join(raw_test[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Traditional Chinese characters to Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['许多', '社区', '长青', '学苑', '多', '开设', '有', '书法', '、', '插花', '、', '土风舞', '班', '，']\n"
     ]
    }
   ],
   "source": [
    "from hanziconv.hanziconv import HanziConv\n",
    "\n",
    "print(list(jieba.cut(HanziConv.toSimplified(\"\".join(raw_test[0])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split sentence to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "許多社區長青學苑多開設有書法、插花、土風舞班，\n",
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞', '班', '，']\n"
     ]
    }
   ],
   "source": [
    "def restore(text, toks):\n",
    "    results = []\n",
    "    offset = 0\n",
    "    for tok in toks:\n",
    "        results.append(text[offset:offset + len(tok)])\n",
    "        offset += len(tok)\n",
    "    return results\n",
    "\n",
    "text = \"\".join(raw_test[0])\n",
    "print(text)\n",
    "print(restore(text, list(jieba.cut(HanziConv.toSimplified(text)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Our Own Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a list of words to a sequence of tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許', '多', '社', '區', '長', '青', '學', '苑', '多', '開', '設', '有', '書', '法', '、', '插', '花', '、', '土', '風', '舞', '班', '，']\n",
      "['L', 'R', 'L', 'R', 'L', 'R', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'R', 'S', 'L', 'M', 'M', 'R', 'S']\n"
     ]
    }
   ],
   "source": [
    "def words_to_tags(words):\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:\n",
    "            tags.append('S')\n",
    "        else:\n",
    "            for i in range(len(word)):\n",
    "                if i == 0:\n",
    "                    tags.append('L')\n",
    "                elif i == len(word) - 1:\n",
    "                    tags.append('R')\n",
    "                else:\n",
    "                    tags.append('M')\n",
    "    return tags\n",
    "    \n",
    "train_X = []\n",
    "train_Y = []\n",
    "\n",
    "test_X = []\n",
    "test_Y = []\n",
    "\n",
    "for sent in raw_train:\n",
    "    train_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence as a sequence of characters\n",
    "    train_Y.append(words_to_tags(sent))\n",
    "    \n",
    "for sent in raw_test:\n",
    "    test_X.append(list(\"\".join(sent)))  # Make the unsegmented sentence\n",
    "    test_Y.append(words_to_tags(sent))\n",
    "    \n",
    "print(test_X[0])\n",
    "print(test_Y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a CRF model for word segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  add word position information and bigram trigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_feature(sent, position):\n",
    "    feature_dict = {'bias' : 1,\n",
    "                    'if_start_word' : 1 if position==0 else 0,\n",
    "                    'if_end_word' : 1 if position==len(sent)-1 else 0,\n",
    "                    'bigram_start_with_x': ''.join(sent[position:position+2]),\n",
    "                    'bigram_end_with_x': ''.join(sent[position-1:position+1]) if position > 0 else '<SOS>'+sent[position] ,\n",
    "                    'trigram_strat_with_x': ''.join(sent[position:position+3]),\n",
    "                    'trigram_middle_with_x': ''.join(sent[position-1:position+2]) if position >0 else '<SOS>'+''.join(sent[position:position+2])\n",
    "                   }\n",
    "    if position > 1 :\n",
    "         feature_dict['trigram_end_with_x'] = ''.join(sent[position-2:position+1]) \n",
    "    elif position > 0 :\n",
    "        feature_dict['trigram_end_with_x'] = '<SOS>' + ''.join(sent[position-1:position+1])\n",
    "    else :\n",
    "        feature_dict['trigram_end_with_x'] = '<SOS>' + '<2OS>' + sent[position]\n",
    "    return feature_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect surround word and add above features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers, metrics\n",
    "\n",
    "def extract_sent_features(x):\n",
    "    sent_features = []\n",
    "    for i in range(len(x)):\n",
    "        tmp = {}\n",
    "        tmp.update(extract_char_features(x, i))\n",
    "        tmp.update(my_feature(x, i))\n",
    "        sent_features.append(tmp)    \n",
    "     \n",
    "    return sent_features\n",
    "\n",
    "  \n",
    "def extract_char_features(sent, position):\n",
    "    char_features = {}\n",
    "    for i in range(-3, 4):\n",
    "        if len(sent) > position + i >= 0:\n",
    "            char_features['char_at_%d' % i] = sent[position + i]\n",
    "    return char_features\n",
    "\n",
    "crf_tagger = sklearn_crfsuite.CRF(algorithm='lbfgs', min_freq=20, max_iterations=300, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'char_at_0': '時',\n",
       "  'char_at_1': '間',\n",
       "  'char_at_2': '：',\n",
       "  'bias': 1,\n",
       "  'if_start_word': 1,\n",
       "  'if_end_word': 0,\n",
       "  'bigram_start_with_x': '時間',\n",
       "  'bigram_end_with_x': '<SOS>時',\n",
       "  'trigram_strat_with_x': '時間：',\n",
       "  'trigram_middle_with_x': '<SOS>時間',\n",
       "  'trigram_end_with_x': '<SOS><2OS>時'},\n",
       " {'char_at_-1': '時',\n",
       "  'char_at_0': '間',\n",
       "  'char_at_1': '：',\n",
       "  'bias': 1,\n",
       "  'if_start_word': 0,\n",
       "  'if_end_word': 0,\n",
       "  'bigram_start_with_x': '間：',\n",
       "  'bigram_end_with_x': '時間',\n",
       "  'trigram_strat_with_x': '間：',\n",
       "  'trigram_middle_with_x': '時間：',\n",
       "  'trigram_end_with_x': '<SOS>時間'},\n",
       " {'char_at_-2': '時',\n",
       "  'char_at_-1': '間',\n",
       "  'char_at_0': '：',\n",
       "  'bias': 1,\n",
       "  'if_start_word': 0,\n",
       "  'if_end_word': 1,\n",
       "  'bigram_start_with_x': '：',\n",
       "  'bigram_end_with_x': '間：',\n",
       "  'trigram_strat_with_x': '：',\n",
       "  'trigram_middle_with_x': '間：',\n",
       "  'trigram_end_with_x': '時間：'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sent_features(train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f124c2dd75c54769a6859801378f8184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=708953), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|█████████████████████████████████████| 708953/708953 [03:23<00:00, 3475.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 20.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 0\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 289847\n",
      "Seconds required: 54.157\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 0.000000\n",
      "c2: 1.000000\n",
      "num_memories: 6\n",
      "max_iterations: 300\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=8.62  loss=9268754.59 active=289679 feature_norm=1.00\n",
      "Iter 2   time=4.36  loss=7087119.33 active=289847 feature_norm=2.72\n",
      "Iter 3   time=4.27  loss=6792615.70 active=289847 feature_norm=4.77\n",
      "Iter 4   time=4.29  loss=5857999.87 active=289847 feature_norm=5.28\n",
      "Iter 5   time=8.52  loss=5125039.60 active=289847 feature_norm=6.27\n",
      "Iter 6   time=8.63  loss=4400907.48 active=289847 feature_norm=7.50\n",
      "Iter 7   time=8.61  loss=3888464.28 active=289847 feature_norm=9.20\n",
      "Iter 8   time=4.51  loss=3496175.15 active=289847 feature_norm=11.64\n",
      "Iter 9   time=4.28  loss=3036294.16 active=289847 feature_norm=15.55\n",
      "Iter 10  time=4.27  loss=2805201.07 active=289847 feature_norm=19.55\n",
      "Iter 11  time=4.29  loss=2557577.50 active=289847 feature_norm=25.99\n",
      "Iter 12  time=4.34  loss=2475565.55 active=289847 feature_norm=33.82\n",
      "Iter 13  time=4.35  loss=2380938.58 active=289847 feature_norm=35.90\n",
      "Iter 14  time=4.33  loss=2359409.46 active=289847 feature_norm=37.65\n",
      "Iter 15  time=4.29  loss=2323095.16 active=289847 feature_norm=40.84\n",
      "Iter 16  time=4.29  loss=2273680.86 active=289847 feature_norm=44.11\n",
      "Iter 17  time=4.29  loss=2165641.71 active=289847 feature_norm=47.88\n",
      "Iter 18  time=4.36  loss=2072774.07 active=289847 feature_norm=51.10\n",
      "Iter 19  time=4.62  loss=2018597.34 active=289847 feature_norm=48.56\n",
      "Iter 20  time=4.50  loss=1965503.14 active=289847 feature_norm=44.43\n",
      "Iter 21  time=4.30  loss=1949284.07 active=289847 feature_norm=43.15\n",
      "Iter 22  time=4.30  loss=1919654.63 active=289847 feature_norm=43.50\n",
      "Iter 23  time=4.31  loss=1849993.54 active=289847 feature_norm=47.09\n",
      "Iter 24  time=4.28  loss=1795765.57 active=289847 feature_norm=51.09\n",
      "Iter 25  time=4.36  loss=1729521.89 active=289847 feature_norm=59.77\n",
      "Iter 26  time=4.36  loss=1683530.72 active=289847 feature_norm=65.44\n",
      "Iter 27  time=4.32  loss=1665880.90 active=289847 feature_norm=65.89\n",
      "Iter 28  time=4.51  loss=1626580.68 active=289847 feature_norm=66.86\n",
      "Iter 29  time=9.10  loss=1613878.53 active=289847 feature_norm=66.00\n",
      "Iter 30  time=4.24  loss=1601211.00 active=289847 feature_norm=65.39\n",
      "Iter 31  time=4.24  loss=1550850.34 active=289847 feature_norm=63.92\n",
      "Iter 32  time=4.24  loss=1523386.68 active=289847 feature_norm=64.16\n",
      "Iter 33  time=4.28  loss=1487792.06 active=289847 feature_norm=64.44\n",
      "Iter 34  time=4.29  loss=1454369.06 active=289847 feature_norm=66.97\n",
      "Iter 35  time=4.23  loss=1410772.85 active=289847 feature_norm=71.15\n",
      "Iter 36  time=4.22  loss=1367606.96 active=289847 feature_norm=74.94\n",
      "Iter 37  time=4.23  loss=1364218.45 active=289847 feature_norm=78.44\n",
      "Iter 38  time=4.29  loss=1344778.98 active=289847 feature_norm=78.21\n",
      "Iter 39  time=4.26  loss=1340758.15 active=289847 feature_norm=77.51\n",
      "Iter 40  time=4.23  loss=1326176.62 active=289847 feature_norm=76.60\n",
      "Iter 41  time=4.23  loss=1300871.66 active=289847 feature_norm=76.93\n",
      "Iter 42  time=4.36  loss=1277895.13 active=289847 feature_norm=80.20\n",
      "Iter 43  time=4.26  loss=1250578.27 active=289847 feature_norm=82.93\n",
      "Iter 44  time=4.29  loss=1237632.86 active=289847 feature_norm=84.59\n",
      "Iter 45  time=4.27  loss=1226386.60 active=289847 feature_norm=88.32\n",
      "Iter 46  time=8.91  loss=1221966.19 active=289847 feature_norm=89.15\n",
      "Iter 47  time=4.25  loss=1217774.40 active=289847 feature_norm=89.63\n",
      "Iter 48  time=4.28  loss=1186950.55 active=289847 feature_norm=92.27\n",
      "Iter 49  time=4.27  loss=1174539.40 active=289847 feature_norm=92.19\n",
      "Iter 50  time=4.24  loss=1151705.12 active=289847 feature_norm=92.88\n",
      "Iter 51  time=4.29  loss=1131338.78 active=289847 feature_norm=93.56\n",
      "Iter 52  time=4.25  loss=1115530.16 active=289847 feature_norm=94.97\n",
      "Iter 53  time=4.23  loss=1104999.14 active=289847 feature_norm=95.71\n",
      "Iter 54  time=8.45  loss=1101560.64 active=289847 feature_norm=95.92\n",
      "Iter 55  time=4.33  loss=1097679.21 active=289847 feature_norm=96.41\n",
      "Iter 56  time=4.33  loss=1096004.59 active=289847 feature_norm=96.50\n",
      "Iter 57  time=4.25  loss=1078821.85 active=289847 feature_norm=97.28\n",
      "Iter 58  time=4.23  loss=1065693.98 active=289847 feature_norm=97.94\n",
      "Iter 59  time=4.23  loss=1053617.60 active=289847 feature_norm=100.47\n",
      "Iter 60  time=4.23  loss=1031310.92 active=289847 feature_norm=100.81\n",
      "Iter 61  time=4.33  loss=1023464.20 active=289847 feature_norm=100.91\n",
      "Iter 62  time=4.28  loss=1021996.00 active=289847 feature_norm=101.50\n",
      "Iter 63  time=4.28  loss=1018256.51 active=289847 feature_norm=101.59\n",
      "Iter 64  time=4.23  loss=1016682.04 active=289847 feature_norm=101.82\n",
      "Iter 65  time=4.23  loss=1011888.55 active=289847 feature_norm=102.66\n",
      "Iter 66  time=4.23  loss=1007551.08 active=289847 feature_norm=103.51\n",
      "Iter 67  time=4.22  loss=996836.71 active=289847 feature_norm=105.36\n",
      "Iter 68  time=8.53  loss=989329.46 active=289847 feature_norm=107.29\n",
      "Iter 69  time=4.24  loss=978798.53 active=289847 feature_norm=108.86\n",
      "Iter 70  time=4.36  loss=969764.28 active=289847 feature_norm=109.54\n",
      "Iter 71  time=4.29  loss=960683.29 active=289847 feature_norm=110.22\n",
      "Iter 72  time=8.60  loss=957063.86 active=289847 feature_norm=111.17\n",
      "Iter 73  time=4.23  loss=949291.04 active=289847 feature_norm=111.52\n",
      "Iter 74  time=4.24  loss=942365.86 active=289847 feature_norm=112.23\n",
      "Iter 75  time=4.25  loss=933661.87 active=289847 feature_norm=113.85\n",
      "Iter 76  time=4.31  loss=931569.45 active=289847 feature_norm=116.93\n",
      "Iter 77  time=4.25  loss=923721.40 active=289847 feature_norm=116.85\n",
      "Iter 78  time=4.25  loss=918551.55 active=289847 feature_norm=117.20\n",
      "Iter 79  time=4.37  loss=911563.81 active=289847 feature_norm=118.82\n",
      "Iter 80  time=4.26  loss=908505.34 active=289847 feature_norm=120.55\n",
      "Iter 81  time=4.43  loss=903556.20 active=289847 feature_norm=121.82\n",
      "Iter 82  time=4.26  loss=901250.59 active=289847 feature_norm=122.30\n",
      "Iter 83  time=4.23  loss=896686.59 active=289847 feature_norm=123.42\n",
      "Iter 84  time=4.31  loss=889586.20 active=289847 feature_norm=125.45\n",
      "Iter 85  time=4.35  loss=879959.31 active=289847 feature_norm=129.10\n",
      "Iter 86  time=4.23  loss=868947.79 active=289847 feature_norm=133.17\n",
      "Iter 87  time=4.23  loss=862828.79 active=289847 feature_norm=134.00\n",
      "Iter 88  time=4.25  loss=858218.24 active=289847 feature_norm=133.98\n",
      "Iter 89  time=4.23  loss=853290.37 active=289847 feature_norm=134.57\n",
      "Iter 90  time=4.23  loss=845905.18 active=289847 feature_norm=135.91\n",
      "Iter 91  time=4.24  loss=840501.63 active=289847 feature_norm=137.82\n",
      "Iter 92  time=4.35  loss=833942.33 active=289847 feature_norm=139.33\n",
      "Iter 93  time=4.26  loss=828606.63 active=289847 feature_norm=141.85\n",
      "Iter 94  time=4.24  loss=824371.59 active=289847 feature_norm=142.75\n",
      "Iter 95  time=4.26  loss=820228.51 active=289847 feature_norm=141.86\n",
      "Iter 96  time=4.26  loss=818436.47 active=289847 feature_norm=141.63\n",
      "Iter 97  time=4.30  loss=814636.89 active=289847 feature_norm=141.90\n",
      "Iter 98  time=4.24  loss=809702.04 active=289847 feature_norm=142.66\n",
      "Iter 99  time=8.64  loss=806271.78 active=289847 feature_norm=143.37\n",
      "Iter 100 time=4.25  loss=801565.40 active=289847 feature_norm=144.72\n",
      "Iter 101 time=4.22  loss=797585.44 active=289847 feature_norm=146.26\n",
      "Iter 102 time=4.23  loss=792794.11 active=289847 feature_norm=147.98\n",
      "Iter 103 time=4.22  loss=789326.37 active=289847 feature_norm=149.04\n",
      "Iter 104 time=4.26  loss=786533.63 active=289847 feature_norm=148.74\n",
      "Iter 105 time=4.26  loss=783632.57 active=289847 feature_norm=148.37\n",
      "Iter 106 time=4.29  loss=781317.35 active=289847 feature_norm=148.45\n",
      "Iter 107 time=8.44  loss=777769.02 active=289847 feature_norm=150.32\n",
      "Iter 108 time=4.23  loss=772638.66 active=289847 feature_norm=151.21\n",
      "Iter 109 time=4.23  loss=770590.79 active=289847 feature_norm=152.04\n",
      "Iter 110 time=4.46  loss=768396.14 active=289847 feature_norm=153.35\n",
      "Iter 111 time=4.22  loss=766213.87 active=289847 feature_norm=154.51\n",
      "Iter 112 time=4.23  loss=761280.60 active=289847 feature_norm=157.91\n",
      "Iter 113 time=4.41  loss=756202.16 active=289847 feature_norm=160.42\n",
      "Iter 114 time=4.38  loss=753830.26 active=289847 feature_norm=159.91\n",
      "Iter 115 time=4.23  loss=751780.30 active=289847 feature_norm=159.40\n",
      "Iter 116 time=4.23  loss=749801.94 active=289847 feature_norm=158.98\n",
      "Iter 117 time=8.52  loss=747353.02 active=289847 feature_norm=159.56\n",
      "Iter 118 time=4.34  loss=742400.03 active=289847 feature_norm=160.10\n",
      "Iter 119 time=4.34  loss=732980.03 active=289847 feature_norm=162.58\n",
      "Iter 120 time=4.28  loss=731680.15 active=289847 feature_norm=164.73\n",
      "Iter 121 time=4.24  loss=728337.18 active=289847 feature_norm=165.52\n",
      "Iter 122 time=4.22  loss=727179.08 active=289847 feature_norm=165.95\n",
      "Iter 123 time=4.22  loss=725487.72 active=289847 feature_norm=166.91\n",
      "Iter 124 time=4.27  loss=723456.82 active=289847 feature_norm=168.18\n",
      "Iter 125 time=4.25  loss=721939.17 active=289847 feature_norm=169.98\n",
      "Iter 126 time=4.22  loss=718514.65 active=289847 feature_norm=171.09\n",
      "Iter 127 time=4.24  loss=716356.30 active=289847 feature_norm=171.37\n",
      "Iter 128 time=8.57  loss=713442.59 active=289847 feature_norm=172.69\n",
      "Iter 129 time=4.23  loss=710446.82 active=289847 feature_norm=173.85\n",
      "Iter 130 time=4.23  loss=709198.62 active=289847 feature_norm=174.77\n",
      "Iter 131 time=4.23  loss=706328.42 active=289847 feature_norm=177.00\n",
      "Iter 132 time=4.28  loss=703075.79 active=289847 feature_norm=179.33\n",
      "Iter 133 time=4.27  loss=701962.78 active=289847 feature_norm=183.56\n",
      "Iter 134 time=4.35  loss=695146.82 active=289847 feature_norm=184.96\n",
      "Iter 135 time=4.28  loss=693074.35 active=289847 feature_norm=184.92\n",
      "Iter 136 time=4.23  loss=691494.82 active=289847 feature_norm=184.86\n",
      "Iter 137 time=4.22  loss=690824.40 active=289847 feature_norm=184.98\n",
      "Iter 138 time=4.22  loss=689522.15 active=289847 feature_norm=185.22\n",
      "Iter 139 time=4.31  loss=688009.03 active=289847 feature_norm=185.61\n",
      "Iter 140 time=4.45  loss=686579.08 active=289847 feature_norm=186.29\n",
      "Iter 141 time=4.22  loss=684493.59 active=289847 feature_norm=187.18\n",
      "Iter 142 time=4.28  loss=682026.15 active=289847 feature_norm=189.44\n",
      "Iter 143 time=4.37  loss=677696.99 active=289847 feature_norm=190.48\n",
      "Iter 144 time=4.25  loss=675704.64 active=289847 feature_norm=190.66\n",
      "Iter 145 time=4.23  loss=673897.38 active=289847 feature_norm=191.34\n",
      "Iter 146 time=8.44  loss=672453.21 active=289847 feature_norm=192.01\n",
      "Iter 147 time=4.23  loss=670306.86 active=289847 feature_norm=192.77\n",
      "Iter 148 time=4.24  loss=662806.50 active=289847 feature_norm=196.28\n",
      "Iter 149 time=4.28  loss=659339.40 active=289847 feature_norm=198.63\n",
      "Iter 150 time=4.28  loss=655669.12 active=289847 feature_norm=200.91\n",
      "Iter 151 time=8.54  loss=654185.64 active=289847 feature_norm=202.24\n",
      "Iter 152 time=4.23  loss=651757.34 active=289847 feature_norm=203.86\n",
      "Iter 153 time=4.23  loss=650499.05 active=289847 feature_norm=204.10\n",
      "Iter 154 time=8.63  loss=649161.18 active=289847 feature_norm=204.64\n",
      "Iter 155 time=4.34  loss=647230.57 active=289847 feature_norm=205.02\n",
      "Iter 156 time=4.29  loss=645660.45 active=289847 feature_norm=205.42\n",
      "Iter 157 time=4.25  loss=643197.29 active=289847 feature_norm=206.33\n",
      "Iter 158 time=4.22  loss=639896.33 active=289847 feature_norm=207.51\n",
      "Iter 159 time=4.22  loss=638973.19 active=289847 feature_norm=209.13\n",
      "Iter 160 time=4.27  loss=636398.42 active=289847 feature_norm=208.78\n",
      "Iter 161 time=4.36  loss=635468.76 active=289847 feature_norm=208.62\n",
      "Iter 162 time=4.25  loss=634190.68 active=289847 feature_norm=208.68\n",
      "Iter 163 time=4.30  loss=631459.81 active=289847 feature_norm=209.31\n",
      "Iter 164 time=8.64  loss=630786.40 active=289847 feature_norm=209.79\n",
      "Iter 165 time=4.22  loss=629119.09 active=289847 feature_norm=210.70\n",
      "Iter 166 time=4.22  loss=627852.72 active=289847 feature_norm=211.48\n",
      "Iter 167 time=4.27  loss=625999.81 active=289847 feature_norm=212.78\n",
      "Iter 168 time=4.24  loss=624310.31 active=289847 feature_norm=213.52\n",
      "Iter 169 time=8.56  loss=623176.55 active=289847 feature_norm=213.85\n",
      "Iter 170 time=4.25  loss=620866.52 active=289847 feature_norm=214.62\n",
      "Iter 171 time=4.22  loss=619529.51 active=289847 feature_norm=214.75\n",
      "Iter 172 time=4.24  loss=617355.04 active=289847 feature_norm=215.16\n",
      "Iter 173 time=8.53  loss=616631.38 active=289847 feature_norm=215.42\n",
      "Iter 174 time=4.28  loss=615166.22 active=289847 feature_norm=215.74\n",
      "Iter 175 time=4.28  loss=613556.43 active=289847 feature_norm=216.18\n",
      "Iter 176 time=4.28  loss=610997.89 active=289847 feature_norm=217.15\n",
      "Iter 177 time=4.24  loss=610346.47 active=289847 feature_norm=218.47\n",
      "Iter 178 time=4.23  loss=607723.57 active=289847 feature_norm=218.83\n",
      "Iter 179 time=4.21  loss=606718.67 active=289847 feature_norm=219.01\n",
      "Iter 180 time=4.24  loss=605692.88 active=289847 feature_norm=219.45\n",
      "Iter 181 time=4.39  loss=605539.76 active=289847 feature_norm=220.20\n",
      "Iter 182 time=4.22  loss=604338.10 active=289847 feature_norm=220.28\n",
      "Iter 183 time=4.30  loss=603698.35 active=289847 feature_norm=220.34\n",
      "Iter 184 time=4.29  loss=602551.49 active=289847 feature_norm=220.71\n",
      "Iter 185 time=4.22  loss=601338.22 active=289847 feature_norm=221.30\n",
      "Iter 186 time=4.22  loss=599608.91 active=289847 feature_norm=222.56\n",
      "Iter 187 time=4.22  loss=598150.07 active=289847 feature_norm=223.36\n",
      "Iter 188 time=4.23  loss=596963.20 active=289847 feature_norm=223.31\n",
      "Iter 189 time=4.75  loss=595719.61 active=289847 feature_norm=223.53\n",
      "Iter 190 time=4.29  loss=594740.47 active=289847 feature_norm=223.85\n",
      "Iter 191 time=4.27  loss=593556.85 active=289847 feature_norm=224.53\n",
      "Iter 192 time=4.23  loss=593080.34 active=289847 feature_norm=224.81\n",
      "Iter 193 time=4.37  loss=592422.82 active=289847 feature_norm=225.11\n",
      "Iter 194 time=4.23  loss=590759.16 active=289847 feature_norm=225.91\n",
      "Iter 195 time=4.22  loss=588560.28 active=289847 feature_norm=227.05\n",
      "Iter 196 time=4.25  loss=586725.95 active=289847 feature_norm=228.71\n",
      "Iter 197 time=4.25  loss=585498.47 active=289847 feature_norm=228.18\n",
      "Iter 198 time=4.27  loss=585050.28 active=289847 feature_norm=228.08\n",
      "Iter 199 time=4.36  loss=583933.93 active=289847 feature_norm=227.81\n",
      "Iter 200 time=4.23  loss=583133.14 active=289847 feature_norm=227.86\n",
      "Iter 201 time=4.27  loss=581902.35 active=289847 feature_norm=228.25\n",
      "Iter 202 time=4.29  loss=581139.30 active=289847 feature_norm=228.71\n",
      "Iter 203 time=4.25  loss=580110.85 active=289847 feature_norm=229.52\n",
      "Iter 204 time=4.22  loss=579323.60 active=289847 feature_norm=230.80\n",
      "Iter 205 time=4.22  loss=578127.85 active=289847 feature_norm=230.62\n",
      "Iter 206 time=4.29  loss=577451.74 active=289847 feature_norm=230.58\n",
      "Iter 207 time=4.22  loss=576535.81 active=289847 feature_norm=230.76\n",
      "Iter 208 time=4.29  loss=575715.33 active=289847 feature_norm=232.04\n",
      "Iter 209 time=4.25  loss=572822.36 active=289847 feature_norm=232.32\n",
      "Iter 210 time=4.24  loss=571519.84 active=289847 feature_norm=232.84\n",
      "Iter 211 time=4.35  loss=570647.47 active=289847 feature_norm=233.67\n",
      "Iter 212 time=4.23  loss=569964.13 active=289847 feature_norm=234.13\n",
      "Iter 213 time=4.23  loss=568307.86 active=289847 feature_norm=235.09\n",
      "Iter 214 time=4.35  loss=566368.08 active=289847 feature_norm=236.22\n",
      "Iter 215 time=8.53  loss=565402.38 active=289847 feature_norm=237.08\n",
      "Iter 216 time=4.23  loss=564441.83 active=289847 feature_norm=237.55\n",
      "Iter 217 time=4.22  loss=564021.80 active=289847 feature_norm=237.41\n",
      "Iter 218 time=4.23  loss=563693.95 active=289847 feature_norm=237.30\n",
      "Iter 219 time=4.22  loss=562576.03 active=289847 feature_norm=237.46\n",
      "Iter 220 time=4.27  loss=561232.62 active=289847 feature_norm=237.85\n",
      "Iter 221 time=4.27  loss=560528.91 active=289847 feature_norm=238.26\n",
      "Iter 222 time=4.24  loss=559529.82 active=289847 feature_norm=239.17\n",
      "Iter 223 time=4.33  loss=559184.59 active=289847 feature_norm=239.83\n",
      "Iter 224 time=4.26  loss=558454.14 active=289847 feature_norm=240.17\n",
      "Iter 225 time=4.24  loss=558146.31 active=289847 feature_norm=240.18\n",
      "Iter 226 time=4.23  loss=557886.63 active=289847 feature_norm=240.16\n",
      "Iter 227 time=4.29  loss=557293.11 active=289847 feature_norm=240.10\n",
      "Iter 228 time=4.51  loss=556347.28 active=289847 feature_norm=240.47\n",
      "Iter 229 time=4.35  loss=555080.13 active=289847 feature_norm=240.62\n",
      "Iter 230 time=4.24  loss=554297.41 active=289847 feature_norm=240.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 231 time=4.29  loss=553674.15 active=289847 feature_norm=240.91\n",
      "Iter 232 time=4.48  loss=552944.68 active=289847 feature_norm=241.08\n",
      "Iter 233 time=4.22  loss=551903.46 active=289847 feature_norm=241.28\n",
      "Iter 234 time=4.45  loss=551159.10 active=289847 feature_norm=241.36\n",
      "Iter 235 time=4.31  loss=550774.49 active=289847 feature_norm=241.40\n",
      "Iter 236 time=4.38  loss=550128.56 active=289847 feature_norm=241.48\n",
      "Iter 237 time=4.29  loss=549170.11 active=289847 feature_norm=241.72\n",
      "Iter 238 time=4.40  loss=548250.87 active=289847 feature_norm=241.90\n",
      "Iter 239 time=4.26  loss=547882.65 active=289847 feature_norm=241.89\n",
      "Iter 240 time=4.26  loss=547533.59 active=289847 feature_norm=241.91\n",
      "Iter 241 time=8.44  loss=547371.02 active=289847 feature_norm=241.95\n",
      "Iter 242 time=4.26  loss=547025.73 active=289847 feature_norm=241.99\n",
      "Iter 243 time=4.32  loss=545617.16 active=289847 feature_norm=242.18\n",
      "Iter 244 time=4.28  loss=544725.21 active=289847 feature_norm=242.35\n",
      "Iter 245 time=4.26  loss=543863.77 active=289847 feature_norm=242.50\n",
      "Iter 246 time=4.22  loss=543747.35 active=289847 feature_norm=242.68\n",
      "Iter 247 time=4.38  loss=542874.22 active=289847 feature_norm=242.65\n",
      "Iter 248 time=4.24  loss=542618.11 active=289847 feature_norm=242.62\n",
      "Iter 249 time=4.25  loss=542296.53 active=289847 feature_norm=242.59\n",
      "Iter 250 time=4.26  loss=542119.72 active=289847 feature_norm=242.65\n",
      "Iter 251 time=4.26  loss=541889.77 active=289847 feature_norm=242.66\n",
      "Iter 252 time=4.22  loss=541650.58 active=289847 feature_norm=242.71\n",
      "Iter 253 time=4.28  loss=541429.23 active=289847 feature_norm=242.79\n",
      "Iter 254 time=4.26  loss=541141.08 active=289847 feature_norm=242.88\n",
      "Iter 255 time=4.30  loss=540808.44 active=289847 feature_norm=243.01\n",
      "Iter 256 time=4.29  loss=540312.06 active=289847 feature_norm=243.08\n",
      "Iter 257 time=4.27  loss=539605.40 active=289847 feature_norm=243.13\n",
      "Iter 258 time=8.65  loss=539170.81 active=289847 feature_norm=243.13\n",
      "Iter 259 time=4.23  loss=538585.71 active=289847 feature_norm=243.13\n",
      "Iter 260 time=4.30  loss=538226.48 active=289847 feature_norm=243.11\n",
      "Iter 261 time=4.28  loss=537747.75 active=289847 feature_norm=243.13\n",
      "Iter 262 time=4.33  loss=537163.80 active=289847 feature_norm=243.19\n",
      "Iter 263 time=4.26  loss=536961.63 active=289847 feature_norm=243.39\n",
      "Iter 264 time=4.29  loss=535959.69 active=289847 feature_norm=243.43\n",
      "Iter 265 time=4.45  loss=535502.22 active=289847 feature_norm=243.51\n",
      "Iter 266 time=4.23  loss=534986.23 active=289847 feature_norm=243.69\n",
      "Iter 267 time=8.43  loss=534833.72 active=289847 feature_norm=243.76\n",
      "Iter 268 time=4.25  loss=534557.70 active=289847 feature_norm=243.88\n",
      "Iter 269 time=4.26  loss=533983.06 active=289847 feature_norm=244.09\n",
      "Iter 270 time=4.28  loss=533218.69 active=289847 feature_norm=244.35\n",
      "Iter 271 time=4.22  loss=532567.13 active=289847 feature_norm=244.58\n",
      "Iter 272 time=4.33  loss=532349.88 active=289847 feature_norm=244.90\n",
      "Iter 273 time=4.34  loss=531588.32 active=289847 feature_norm=245.00\n",
      "Iter 274 time=4.27  loss=531331.88 active=289847 feature_norm=244.97\n",
      "Iter 275 time=4.23  loss=531032.31 active=289847 feature_norm=244.95\n",
      "Iter 276 time=4.23  loss=530771.53 active=289847 feature_norm=245.07\n",
      "Iter 277 time=4.39  loss=530428.87 active=289847 feature_norm=245.21\n",
      "Iter 278 time=4.31  loss=529372.86 active=289847 feature_norm=245.70\n",
      "Iter 279 time=4.27  loss=528651.82 active=289847 feature_norm=246.16\n",
      "Iter 280 time=4.25  loss=528127.44 active=289847 feature_norm=246.66\n",
      "Iter 281 time=4.25  loss=527770.14 active=289847 feature_norm=246.60\n",
      "Iter 282 time=4.29  loss=527184.28 active=289847 feature_norm=246.61\n",
      "Iter 283 time=4.23  loss=526691.96 active=289847 feature_norm=246.77\n",
      "Iter 284 time=4.37  loss=526270.55 active=289847 feature_norm=247.05\n",
      "Iter 285 time=4.23  loss=525940.23 active=289847 feature_norm=247.39\n",
      "Iter 286 time=4.23  loss=525739.79 active=289847 feature_norm=247.43\n",
      "Iter 287 time=4.27  loss=525439.67 active=289847 feature_norm=247.45\n",
      "Iter 288 time=4.36  loss=524783.88 active=289847 feature_norm=247.57\n",
      "Iter 289 time=4.30  loss=523859.62 active=289847 feature_norm=248.18\n",
      "Iter 290 time=8.42  loss=523522.15 active=289847 feature_norm=248.54\n",
      "Iter 291 time=4.22  loss=523299.92 active=289847 feature_norm=248.79\n",
      "Iter 292 time=4.23  loss=523074.58 active=289847 feature_norm=249.02\n",
      "Iter 293 time=4.25  loss=522690.87 active=289847 feature_norm=249.26\n",
      "Iter 294 time=4.40  loss=522213.85 active=289847 feature_norm=249.93\n",
      "Iter 295 time=4.43  loss=521660.20 active=289847 feature_norm=249.88\n",
      "Iter 296 time=4.22  loss=521150.75 active=289847 feature_norm=249.78\n",
      "Iter 297 time=4.25  loss=520343.15 active=289847 feature_norm=249.58\n",
      "Iter 298 time=4.37  loss=520031.40 active=289847 feature_norm=249.77\n",
      "Iter 299 time=4.22  loss=519665.04 active=289847 feature_norm=249.87\n",
      "Iter 300 time=4.26  loss=519380.90 active=289847 feature_norm=249.99\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 1387.415\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 289847 (289847)\n",
      "Number of active attributes: 227856 (10541211)\n",
      "Number of active labels: 4 (4)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.434\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_states=None, all_possible_transitions=None,\n",
       "    averaging=None, c=None, c1=None, c2=None, calibration_candidates=None,\n",
       "    calibration_eta=None, calibration_max_trials=None, calibration_rate=None,\n",
       "    calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "    gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=300,\n",
       "    max_linesearch=None, min_freq=20, model_filename=None, num_memories=None,\n",
       "    pa_type=None, period=None, trainer_cls=None, variance=None, verbose=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_X = []\n",
    "for x in tqdm(train_X):\n",
    "    feature_X.append(extract_sent_features(x))\n",
    "crf_tagger.fit(feature_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['法國', '總統', '馬克宏', '已', '到', '現場', '勘災', '，', '初步', '傳出', '火警', '可能', '與', '目前', '聖母院', '的', '維修', '工程', '有關', '。']\n"
     ]
    }
   ],
   "source": [
    "def segment(sent):\n",
    "    tags = crf_tagger.predict_single(extract_sent_features(list(sent)))\n",
    "    tokens = []\n",
    "    tok = \"\"\n",
    "    for ch, tag in zip(list(sent), tags):\n",
    "        if tag in ['S', 'L'] and tok != \"\":\n",
    "            tokens.append(tok)\n",
    "            tok = \"\"\n",
    "        tok += ch\n",
    "    if tok:\n",
    "        tokens.append(tok)\n",
    "    return tokens\n",
    "            \n",
    "print(segment(\"法國總統馬克宏已到現場勘災，初步傳出火警可能與目前聖母院的維修工程有關。\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer for CWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(actual_toks, pred_toks):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    p = 0\n",
    "    q = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    while i < len(actual_toks) and j < len(pred_toks):\n",
    "        if p == q:\n",
    "            if actual_toks[i] == pred_toks[j]:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            p += len(actual_toks[i])\n",
    "            q += len(pred_toks[j])\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif p < q:\n",
    "            p += len(actual_toks[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "            q += len(pred_toks[j])\n",
    "            j += 1\n",
    "    return tp, fp, len(actual_toks)\n",
    "    \n",
    "def score(actual_sents, pred_sents):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total = 0\n",
    "    for actual_toks, pred_toks in zip(actual_sents, pred_sents):\n",
    "        tp_, fp_, total_ = compare(actual_toks, pred_toks)\n",
    "        tp += tp_\n",
    "        fp += fp_\n",
    "        total += total_\n",
    "    recall = float(tp) / total\n",
    "    precision = float(tp) / (tp + fp)\n",
    "    f1 = 2.0 * recall * precision / (recall + precision)\n",
    "    return recall, precision, f1        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "# My model acheived F-score up to  0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "['許多', '社區', '長青', '學苑', '多', '開', '設有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "(0.9362155020551967, 0.9246997414272251, 0.9304219906872698)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "for sent in raw_test:\n",
    "    pred.append(segment(\"\".join(sent)))\n",
    "    actual.append(sent)\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "\n",
    "print(score(actual, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compared with jieba F-score 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞班', '，']\n",
      "['許多', '社區', '長青', '學苑', '多', '開設', '有', '書法', '、', '插花', '、', '土風舞', '班', '，']\n",
      "(0.8149262738957396, 0.8293466352378739, 0.8220732208967503)\n"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "actual = []\n",
    "fout = open(\"jieba.out\", \"w\")\n",
    "for sent in raw_test:\n",
    "    text = \"\".join(sent)\n",
    "    r = list(jieba.cut(HanziConv.toSimplified(text)))\n",
    "    r = restore(text, r)\n",
    "    fout.write(\" \".join(r) + \"\\n\")\n",
    "    pred.append(r)\n",
    "    actual.append(sent)\n",
    "print(actual[0])\n",
    "print(pred[0])\n",
    "\n",
    "print(score(actual, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
